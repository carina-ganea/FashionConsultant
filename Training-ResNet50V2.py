import os
import tensorflow as tf
from keras import Sequential, layers
from keras.applications import resnet
from keras.preprocessing.image import ImageDataGenerator
from keras.applications.resnet_v2 import ResNet50V2, preprocess_input
from keras.callbacks import ModelCheckpoint, LearningRateScheduler
from keras.callbacks import ReduceLROnPlateau
import numpy as np
from keras.optimizer_v2.adam import Adam

img_size = 256
root_dir = 'Train_Data'

x_train = tf.keras.utils.image_dataset_from_directory(root_dir,
                                                      labels="inferred",
                                                      label_mode="categorical",
                                                      class_names=['Blazer', 'Blouse', 'Body', 'Dress', 'Hat', 'Hoodie',
                                                                   'Longsleeve', 'Outwear', 'Pants', 'Polo',
                                                                   'Pullover', 'Shirt', 'Shoes', 'Shorts', 'Skirt',
                                                                   'T-Shirt', 'Top', 'Undershirt'],
                                                      color_mode="rgb",
                                                      batch_size=None,
                                                      image_size=(256, 256),
                                                      shuffle=False,
                                                      seed=1)

datagen = ImageDataGenerator(
    # set input mean to 0 over the dataset
    featurewise_center=True,
    # set each sample mean to 0
    samplewise_center=False,
    # divide inputs by std of dataset
    featurewise_std_normalization=True,
    # divide each input by its std
    samplewise_std_normalization=False,
    # apply ZCA whitening
    zca_whitening=False,
    # epsilon for ZCA whitening
    zca_epsilon=1e-06,
    # randomly rotate images in the range (deg 0 to 180)
    rotation_range=30,
    # randomly shift images horizontally
    width_shift_range=0.1,
    # randomly shift images vertically
    height_shift_range=0.1,
    # set range for random shear
    shear_range=25.0,
    # set range for random zoom
    zoom_range=[1.0, 2.0],
    # set range for random channel shifts
    channel_shift_range=0.,
    # set mode for filling points outside the input boundaries
    fill_mode='nearest',
    # value used for fill_mode = "constant"
    cval=255,
    # randomly flip images
    horizontal_flip=True,
    # randomly flip images
    vertical_flip=False,
    # set rescaling factor (applied before any other transformation)
    rescale=1.0 / 255.0,
    # set function that will be applied on each input
    preprocessing_function=None,
    # image data format, either "channels_first" or "channels_last"
    data_format=None,
    # fraction of images reserved for validation (strictly between 0 and 1)
    validation_split=0.15)

train_gen = datagen.flow_from_directory(
    root_dir,
    class_mode="categorical",
    batch_size=32,
    subset='training',
    shuffle=True)
val_gen = datagen.flow_from_directory(
    root_dir,
    class_mode="categorical",
    batch_size=32,
    subset='validation',
    shuffle=True)

# Prepare model saving directory.
save_dir = os.path.join(os.getcwd(), 'saved_models')
model_name = 'Train_Data/ResNet50V2Final.ep{epoch:03d}.h5'
if not os.path.isdir(save_dir):
    os.makedirs(save_dir)
filepath = os.path.join(save_dir, model_name)

# Prepare callbacks for model saving and for learning rate adjustment.
checkpoint = ModelCheckpoint(filepath=filepath,
                             monitor='val_accuracy',
                             verbose=1,
                             save_best_only=True)


# Setting LR for different number of Epochs
def lr_schedule(epoch):
    lr = 1e-3
    if epoch > 180:
        lr *= 0.5e-3
    elif epoch > 160:
        lr *= 1e-3
    elif epoch > 120:
        lr *= 1e-2
    elif epoch > 80:
        lr *= 1e-1
    print('Learning rate: ', lr)
    return lr


model = ResNet50V2(include_top=False, weights='imagenet', pooling='avg', input_shape=(256, 256, 3))

model1 = Sequential()
model1.add(model)
for layer in model.layers[:len(model.layers) - 9]:
    layer.trainable = False
model.summary()
model1.add(layers.Dense(18, activation='softmax'))

model = model1
model.compile(loss='categorical_crossentropy',
              optimizer=Adam(learning_rate=1e-3),
              metrics=['accuracy'])
model.summary()


lr_scheduler = LearningRateScheduler(lr_schedule)

lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),
                               cooldown=0,
                               patience=5,
                               min_lr=0.5e-9)

callbacks = [checkpoint, lr_reducer, lr_scheduler]

# Fit the model on the batches generated by datagen.flow().
model.fit_generator(train_gen, steps_per_epoch=train_gen.samples // 32,
                    validation_data=val_gen, validation_steps=val_gen.samples // 32,
                    epochs=200, verbose=1, workers=4, callbacks=callbacks)

